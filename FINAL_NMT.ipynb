{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a917d5b8ec5945048654c3b162ee1340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4823988ca2f2407980108765bbbd8940",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b158abe9dcf04a189fe407cab7d8f38c",
              "IPY_MODEL_b184ce5a33814a7ca366360136835308"
            ]
          }
        },
        "4823988ca2f2407980108765bbbd8940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b158abe9dcf04a189fe407cab7d8f38c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_78b09e88558d4be296935a14c488e073",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6429cf509d0f467595ca187001634fe4"
          }
        },
        "b184ce5a33814a7ca366360136835308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_41bededbd99540e6bd2be9be1d1592f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 5.20MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b94b3c66a09544d1ac35f2dd07059e09"
          }
        },
        "78b09e88558d4be296935a14c488e073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6429cf509d0f467595ca187001634fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41bededbd99540e6bd2be9be1d1592f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b94b3c66a09544d1ac35f2dd07059e09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13nSxSUZ2z_B"
      },
      "source": [
        "#Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCknGT4W2kjc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cd68eb44-9dfa-4157-a860-0a0623d10dc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LUJOU_o3l9h"
      },
      "source": [
        "# Loading Pre-Trained BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6zYaDRd3AXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "f0582a3e-34bd-4cde-b43e-72a37b286dcf"
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.48)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.17.48)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.48->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP40osB8USgs"
      },
      "source": [
        "# Load Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0for-zgURrE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "594d503f-95a7-4c58-89da-05d148b81388"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 18.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 25.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=57ef66a21eab5e4b7a9cddfc1fa9cf952d61aa2a534db03e56c4a3975823529e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFWqJYN33znY"
      },
      "source": [
        "#Load the utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U1XSudH3q05"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import numpy as np\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFMj7aVD35NC"
      },
      "source": [
        "path_to_file = \"drive/My Drive/english_telugu_data.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJH2mU8GKy86"
      },
      "source": [
        "pre-process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf1VXEUq38SY"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = w.lower()\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = '[CLS] ' + w + ' [SEP]'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpDcodujLAim"
      },
      "source": [
        "loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLWZyWW64FY9"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('++++$++++')]  for l in lines[:num_examples]]\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abu2-Ssa4Hs8"
      },
      "source": [
        "en, te = create_dataset(path_to_file, num_examples=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgp_6M5SgzYz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7a4ce93e-5439-4739-a42c-6d34f1350a63"
      },
      "source": [
        "en[0],te[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS] his legs are long .  [SEP]',\n",
              " '[CLS] అతని కాళ్ళు పొడవుగా ఉన్నాయి .  [SEP]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-kRVaZGLIEG"
      },
      "source": [
        "#Loading the BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5-Mzptf4SBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ffa7a8a1-f9e7-449a-9a15-7567e94fa2ea"
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
            "100%|██████████| 995526/995526 [00:00<00:00, 12524825.00B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_9ZIavyWm6H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a917d5b8ec5945048654c3b162ee1340",
            "4823988ca2f2407980108765bbbd8940",
            "b158abe9dcf04a189fe407cab7d8f38c",
            "b184ce5a33814a7ca366360136835308",
            "78b09e88558d4be296935a14c488e073",
            "6429cf509d0f467595ca187001634fe4",
            "41bededbd99540e6bd2be9be1d1592f5",
            "b94b3c66a09544d1ac35f2dd07059e09"
          ]
        },
        "outputId": "87fda4b3-10dc-400e-b449-698169d74175"
      },
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "tokenizer2 = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a917d5b8ec5945048654c3b162ee1340",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic46XUgA4T7f"
      },
      "source": [
        "target_tok=[]\n",
        "input_tok=[]\n",
        "input_tensor=[]\n",
        "target_tensor=[]\n",
        "for i in range(len(en)):\n",
        "  tokenized_text1 = tokenizer.tokenize(en[i])\n",
        "  tokenized_text2 = tokenizer.tokenize(te[i])\n",
        "  input_tok.append(tokenized_text1)\n",
        "  target_tok.append(tokenized_text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9kqmP-75Ajw"
      },
      "source": [
        "for j in range(len(en)):\n",
        "  indexed_tokens_eng = tokenizer.convert_tokens_to_ids(input_tok[j])\n",
        "  indexed_tokens_tel = tokenizer.convert_tokens_to_ids(target_tok[j])\n",
        "  input_tensor.append(indexed_tokens_eng)\n",
        "  target_tensor.append(indexed_tokens_tel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPBHkNP55ObX"
      },
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Drx8RD5TXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "839aa11b-99f0-43b2-fe72-585a2fdb9d57"
      },
      "source": [
        "print(max_length(input_tensor))\n",
        "print(max_length(target_tensor))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n",
            "67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOXkHNQT83Wk"
      },
      "source": [
        "max_length_inp=max_length(input_tensor)\n",
        "max_length_targ=max_length(target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m-g-hY25Vym"
      },
      "source": [
        "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,\n",
        "                                                         padding='post')\n",
        "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,\n",
        "                                                         padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmnWVXAQLSM_"
      },
      "source": [
        "splitting the data for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKEujjq55dFz"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbj02DDa508H"
      },
      "source": [
        "def convert(tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%s ----> %s\" % (t, tokenizer.convert_tokens_to_ids([t])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLuofLFp6XXk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "6c37e9b7-8e1b-48f5-d6ff-1a7fc7b3aa32"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(input_tok[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(target_tok[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "[CLS] ----> [101]\n",
            "his ----> [10226]\n",
            "legs ----> [51863]\n",
            "are ----> [10301]\n",
            "long ----> [11695]\n",
            ". ----> [119]\n",
            "[SEP] ----> [102]\n",
            "\n",
            "Target Language; index to word mapping\n",
            "[CLS] ----> [101]\n",
            "అతని ----> [62522]\n",
            "క ----> [1201]\n",
            "##ా ----> [15735]\n",
            "##ళ్ళు ----> [92090]\n",
            "ప ----> [1220]\n",
            "##ొ ----> [111340]\n",
            "##డ ----> [36008]\n",
            "##వు ----> [27850]\n",
            "##గా ----> [12540]\n",
            "ఉన్నాయి ----> [11251]\n",
            ". ----> [119]\n",
            "[SEP] ----> [102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0FUa-ur_909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ccf96d2a-a975-4d6c-ac2d-0ebd9f2f996d"
      },
      "source": [
        "v=np.unique(input_tensor)\n",
        "print(v[-1])\n",
        "w=np.unique(target_tensor)\n",
        "print(w[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "110790\n",
            "111341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buuCx0lg7wxQ"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 64\n",
        "units = 256\n",
        "vocab_inp_size = v[-1]+1\n",
        "vocab_tar_size = w[-1]+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QcTeOlYCjNe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d3f9b89-79fa-4562-bbb3-81b72272361a"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 35]), TensorShape([32, 67]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1YZf0IpLfeb"
      },
      "source": [
        "Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKWzkcz8DA3r"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yeObUysDkEx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "41053b2f-954e-44e2-efb2-1a781a49ccb4"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (32, 35, 256)\n",
            "Encoder Hidden state shape: (batch size, units) (32, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N009DRpYLiSY"
      },
      "source": [
        "Attention method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjaHxkSrEAlX"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3IOsqlREKDn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "56522c05-b3ff-4e5e-a2f0-c0f2b03d3b91"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (32, 256)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (32, 35, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7w2rGb-LmFy"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Fpb7exENkW"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW8NpBU1Ee-B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "22c7ab0b-06be-4055-8081-c3c438e0e5a2"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, st, att = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
        "print ('Decoder output shape: {}'.format(st.shape))\n",
        "print ('Decoder output shape: {}'.format(att.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 111342)\n",
            "Decoder output shape: (32, 256)\n",
            "Decoder output shape: (32, 35, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J17BYJrEjmO"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37nU0KVwLphu"
      },
      "source": [
        "Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vib9jbLFnQK"
      },
      "source": [
        "#@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims(tokenizer.convert_tokens_to_ids(['[CLS]']) * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHdlqd41HbOh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78d0c020-5baf-4d6b-e593-53017fbbdc9b"
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.3983\n",
            "Epoch 1 Batch 100 Loss 1.3814\n",
            "Epoch 1 Batch 200 Loss 1.4441\n",
            "Epoch 1 Loss 1.4792\n",
            "Time taken for 1 epoch 234.63261675834656 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2974\n",
            "Epoch 2 Batch 100 Loss 1.2634\n",
            "Epoch 2 Batch 200 Loss 1.3143\n",
            "Epoch 2 Loss 1.3523\n",
            "Time taken for 1 epoch 232.5425319671631 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.4748\n",
            "Epoch 3 Batch 100 Loss 1.2264\n",
            "Epoch 3 Batch 200 Loss 1.2505\n",
            "Epoch 3 Loss 1.1970\n",
            "Time taken for 1 epoch 230.7812294960022 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0561\n",
            "Epoch 4 Batch 100 Loss 1.1066\n",
            "Epoch 4 Batch 200 Loss 0.9813\n",
            "Epoch 4 Loss 1.0596\n",
            "Time taken for 1 epoch 230.39495611190796 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1285\n",
            "Epoch 5 Batch 100 Loss 1.0407\n",
            "Epoch 5 Batch 200 Loss 1.0009\n",
            "Epoch 5 Loss 0.9901\n",
            "Time taken for 1 epoch 230.4363534450531 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0189\n",
            "Epoch 6 Batch 100 Loss 0.8861\n",
            "Epoch 6 Batch 200 Loss 0.9767\n",
            "Epoch 6 Loss 0.9454\n",
            "Time taken for 1 epoch 234.4301781654358 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9016\n",
            "Epoch 7 Batch 100 Loss 0.9356\n",
            "Epoch 7 Batch 200 Loss 0.8139\n",
            "Epoch 7 Loss 0.9087\n",
            "Time taken for 1 epoch 233.46061873435974 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7449\n",
            "Epoch 8 Batch 100 Loss 0.9255\n",
            "Epoch 8 Batch 200 Loss 0.9580\n",
            "Epoch 8 Loss 0.8742\n",
            "Time taken for 1 epoch 231.2755696773529 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9969\n",
            "Epoch 9 Batch 100 Loss 0.9602\n",
            "Epoch 9 Batch 200 Loss 0.8265\n",
            "Epoch 9 Loss 0.8425\n",
            "Time taken for 1 epoch 229.50443744659424 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.8720\n",
            "Epoch 10 Batch 100 Loss 0.8052\n",
            "Epoch 10 Batch 200 Loss 0.8460\n",
            "Epoch 10 Loss 0.8135\n",
            "Time taken for 1 epoch 229.95010209083557 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.8582\n",
            "Epoch 11 Batch 100 Loss 0.8182\n",
            "Epoch 11 Batch 200 Loss 0.7957\n",
            "Epoch 11 Loss 0.7851\n",
            "Time taken for 1 epoch 228.88698267936707 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.7747\n",
            "Epoch 12 Batch 100 Loss 0.7481\n",
            "Epoch 12 Batch 200 Loss 0.6498\n",
            "Epoch 12 Loss 0.7543\n",
            "Time taken for 1 epoch 228.94461584091187 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.7534\n",
            "Epoch 13 Batch 100 Loss 0.7576\n",
            "Epoch 13 Batch 200 Loss 0.6702\n",
            "Epoch 13 Loss 0.7294\n",
            "Time taken for 1 epoch 228.30069851875305 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.7075\n",
            "Epoch 14 Batch 100 Loss 0.7511\n",
            "Epoch 14 Batch 200 Loss 0.6797\n",
            "Epoch 14 Loss 0.7088\n",
            "Time taken for 1 epoch 229.87065815925598 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.8079\n",
            "Epoch 15 Batch 100 Loss 0.7053\n",
            "Epoch 15 Batch 200 Loss 0.5966\n",
            "Epoch 15 Loss 0.6904\n",
            "Time taken for 1 epoch 228.8979136943817 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.7492\n",
            "Epoch 16 Batch 100 Loss 0.7220\n",
            "Epoch 16 Batch 200 Loss 0.5754\n",
            "Epoch 16 Loss 0.6731\n",
            "Time taken for 1 epoch 229.29989433288574 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.6861\n",
            "Epoch 17 Batch 100 Loss 0.6902\n",
            "Epoch 17 Batch 200 Loss 0.6225\n",
            "Epoch 17 Loss 0.6572\n",
            "Time taken for 1 epoch 229.23608493804932 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.6958\n",
            "Epoch 18 Batch 100 Loss 0.6850\n",
            "Epoch 18 Batch 200 Loss 0.6043\n",
            "Epoch 18 Loss 0.6419\n",
            "Time taken for 1 epoch 229.40865445137024 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.5629\n",
            "Epoch 19 Batch 100 Loss 0.6442\n",
            "Epoch 19 Batch 200 Loss 0.5937\n",
            "Epoch 19 Loss 0.6274\n",
            "Time taken for 1 epoch 228.69541025161743 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.6068\n",
            "Epoch 20 Batch 100 Loss 0.6414\n",
            "Epoch 20 Batch 200 Loss 0.6206\n",
            "Epoch 20 Loss 0.6133\n",
            "Time taken for 1 epoch 228.8967580795288 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.4578\n",
            "Epoch 21 Batch 100 Loss 0.4758\n",
            "Epoch 21 Batch 200 Loss 0.6539\n",
            "Epoch 21 Loss 0.5998\n",
            "Time taken for 1 epoch 230.50862383842468 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.5866\n",
            "Epoch 22 Batch 100 Loss 0.6806\n",
            "Epoch 22 Batch 200 Loss 0.5603\n",
            "Epoch 22 Loss 0.5864\n",
            "Time taken for 1 epoch 231.91390657424927 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.6040\n",
            "Epoch 23 Batch 100 Loss 0.6306\n",
            "Epoch 23 Batch 200 Loss 0.5111\n",
            "Epoch 23 Loss 0.5735\n",
            "Time taken for 1 epoch 231.04286813735962 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.5629\n",
            "Epoch 24 Batch 100 Loss 0.6660\n",
            "Epoch 24 Batch 200 Loss 0.5417\n",
            "Epoch 24 Loss 0.5606\n",
            "Time taken for 1 epoch 232.29051280021667 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.5630\n",
            "Epoch 25 Batch 100 Loss 0.4977\n",
            "Epoch 25 Batch 200 Loss 0.5553\n",
            "Epoch 25 Loss 0.5481\n",
            "Time taken for 1 epoch 230.8679554462433 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.5695\n",
            "Epoch 26 Batch 100 Loss 0.4990\n",
            "Epoch 26 Batch 200 Loss 0.5898\n",
            "Epoch 26 Loss 0.5357\n",
            "Time taken for 1 epoch 229.8210608959198 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.5584\n",
            "Epoch 27 Batch 100 Loss 0.5818\n",
            "Epoch 27 Batch 200 Loss 0.5949\n",
            "Epoch 27 Loss 0.5238\n",
            "Time taken for 1 epoch 228.9251115322113 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.5355\n",
            "Epoch 28 Batch 100 Loss 0.5644\n",
            "Epoch 28 Batch 200 Loss 0.6135\n",
            "Epoch 28 Loss 0.5121\n",
            "Time taken for 1 epoch 228.3469009399414 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.5447\n",
            "Epoch 29 Batch 100 Loss 0.5814\n",
            "Epoch 29 Batch 200 Loss 0.5004\n",
            "Epoch 29 Loss 0.5003\n",
            "Time taken for 1 epoch 228.17743515968323 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.5122\n",
            "Epoch 30 Batch 100 Loss 0.5227\n",
            "Epoch 30 Batch 200 Loss 0.4998\n",
            "Epoch 30 Loss 0.4889\n",
            "Time taken for 1 epoch 228.93235397338867 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.4895\n",
            "Epoch 31 Batch 100 Loss 0.4448\n",
            "Epoch 31 Batch 200 Loss 0.4634\n",
            "Epoch 31 Loss 0.4771\n",
            "Time taken for 1 epoch 228.2677071094513 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.5205\n",
            "Epoch 32 Batch 100 Loss 0.4809\n",
            "Epoch 32 Batch 200 Loss 0.5015\n",
            "Epoch 32 Loss 0.4663\n",
            "Time taken for 1 epoch 228.26446843147278 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.4864\n",
            "Epoch 33 Batch 100 Loss 0.5075\n",
            "Epoch 33 Batch 200 Loss 0.4280\n",
            "Epoch 33 Loss 0.4553\n",
            "Time taken for 1 epoch 228.93328475952148 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.4409\n",
            "Epoch 34 Batch 100 Loss 0.3990\n",
            "Epoch 34 Batch 200 Loss 0.4045\n",
            "Epoch 34 Loss 0.4445\n",
            "Time taken for 1 epoch 229.4698474407196 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.4078\n",
            "Epoch 35 Batch 100 Loss 0.3928\n",
            "Epoch 35 Batch 200 Loss 0.4058\n",
            "Epoch 35 Loss 0.4336\n",
            "Time taken for 1 epoch 228.87687969207764 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.3940\n",
            "Epoch 36 Batch 100 Loss 0.4643\n",
            "Epoch 36 Batch 200 Loss 0.4142\n",
            "Epoch 36 Loss 0.4227\n",
            "Time taken for 1 epoch 228.99051189422607 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.4055\n",
            "Epoch 37 Batch 100 Loss 0.3672\n",
            "Epoch 37 Batch 200 Loss 0.4075\n",
            "Epoch 37 Loss 0.4134\n",
            "Time taken for 1 epoch 230.56256198883057 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.3674\n",
            "Epoch 38 Batch 100 Loss 0.4054\n",
            "Epoch 38 Batch 200 Loss 0.4451\n",
            "Epoch 38 Loss 0.4026\n",
            "Time taken for 1 epoch 233.0507619380951 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.4798\n",
            "Epoch 39 Batch 100 Loss 0.4010\n",
            "Epoch 39 Batch 200 Loss 0.3671\n",
            "Epoch 39 Loss 0.3928\n",
            "Time taken for 1 epoch 230.72821521759033 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.3499\n",
            "Epoch 40 Batch 100 Loss 0.3824\n",
            "Epoch 40 Batch 200 Loss 0.4097\n",
            "Epoch 40 Loss 0.3832\n",
            "Time taken for 1 epoch 230.97538542747498 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.3375\n",
            "Epoch 41 Batch 100 Loss 0.4285\n",
            "Epoch 41 Batch 200 Loss 0.3573\n",
            "Epoch 41 Loss 0.3738\n",
            "Time taken for 1 epoch 230.0423457622528 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.3598\n",
            "Epoch 42 Batch 100 Loss 0.3593\n",
            "Epoch 42 Batch 200 Loss 0.3481\n",
            "Epoch 42 Loss 0.3644\n",
            "Time taken for 1 epoch 238.91806411743164 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.3213\n",
            "Epoch 43 Batch 100 Loss 0.3557\n",
            "Epoch 43 Batch 200 Loss 0.3358\n",
            "Epoch 43 Loss 0.3555\n",
            "Time taken for 1 epoch 238.56826424598694 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.3199\n",
            "Epoch 44 Batch 100 Loss 0.3398\n",
            "Epoch 44 Batch 200 Loss 0.3754\n",
            "Epoch 44 Loss 0.3457\n",
            "Time taken for 1 epoch 237.3839726448059 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.3229\n",
            "Epoch 45 Batch 100 Loss 0.3142\n",
            "Epoch 45 Batch 200 Loss 0.3060\n",
            "Epoch 45 Loss 0.3375\n",
            "Time taken for 1 epoch 236.50832509994507 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.3519\n",
            "Epoch 46 Batch 100 Loss 0.3120\n",
            "Epoch 46 Batch 200 Loss 0.3806\n",
            "Epoch 46 Loss 0.3297\n",
            "Time taken for 1 epoch 232.23059558868408 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.3135\n",
            "Epoch 47 Batch 100 Loss 0.3292\n",
            "Epoch 47 Batch 200 Loss 0.3096\n",
            "Epoch 47 Loss 0.3213\n",
            "Time taken for 1 epoch 230.83341455459595 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.3331\n",
            "Epoch 48 Batch 100 Loss 0.3561\n",
            "Epoch 48 Batch 200 Loss 0.3037\n",
            "Epoch 48 Loss 0.3126\n",
            "Time taken for 1 epoch 231.3511447906494 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.3054\n",
            "Epoch 49 Batch 100 Loss 0.2544\n",
            "Epoch 49 Batch 200 Loss 0.3539\n",
            "Epoch 49 Loss 0.3042\n",
            "Time taken for 1 epoch 229.9028239250183 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3808\n",
            "Epoch 50 Batch 100 Loss 0.2775\n",
            "Epoch 50 Batch 200 Loss 0.3111\n",
            "Epoch 50 Loss 0.2964\n",
            "Time taken for 1 epoch 231.0063304901123 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNhrpCgH1jbz"
      },
      "source": [
        "Evaluation and Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzW0GfICHmZr"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  input_tokens=tokenizer.tokenize(sentence)\n",
        "  inputs=tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "  result2=[]\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims(tokenizer.convert_tokens_to_ids(['[CLS]']), 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    result2.append(predicted_id)\n",
        "\n",
        "    result += str(tokenizer.convert_ids_to_tokens([predicted_id])) + ' '\n",
        "\n",
        "    if tokenizer.convert_ids_to_tokens([predicted_id]) == tokenizer.convert_ids_to_tokens([102]):\n",
        "      return result, sentence, attention_plot, result2\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot, result2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0uHluCq4WT0"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot, result2 = evaluate(sentence)\n",
        "  predicted_text=tokenizer2.decode(result2)\n",
        "  predicted_text=predicted_text.replace(\" [SEP]\",\"\")\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTw9XN2z4dmU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "59da7cbe-6dee-43cd-913e-d6a5eb1a241c"
      },
      "source": [
        "translate(u'She knows nothing about your family.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: [CLS] she knows nothing about your family .  [SEP]\n",
            "Predicted translation: మీ కుటుంబం గురించి ఆమెకు ఎప్పుడూ నాకు తెలియదు.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FbsNlashzYx"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28SeGT3yiV-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8516b80c-3905-4045-b841-45604359c394"
      },
      "source": [
        "reference = [[ \"మీ\", \"కుటుంబం\" ,\"గురించి\" ,\"ఆమెకు\",\"ఏమీ\" ,\"తెలియదు\"]]\n",
        "test = [\"మీ\" ,\"కుటుంబం\" ,\"గురించి\", \"ఆమెకు\", \"ఎప్పుడూ\" ,\"తెలియదు\"]\n",
        "score = sentence_bleu(reference,test)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.537284965911771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hyrrwV-ihty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}